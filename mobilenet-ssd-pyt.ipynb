{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10663728,"sourceType":"datasetVersion","datasetId":6604220},{"sourceId":10670029,"sourceType":"datasetVersion","datasetId":6608619},{"sourceId":10670070,"sourceType":"datasetVersion","datasetId":6608653}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\n\n# Check PyTorch version\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Check if ssdlite320_mobilenet_v3_large is available\ntry:\n    model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n    print(\"ssdlite320_mobilenet_v3_large model is available\")\nexcept AttributeError:\n    print(\"ssdlite320_mobilenet_v3_large model is not available in this version of torchvision\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:00:27.693456Z","iopub.execute_input":"2025-02-07T14:00:27.693769Z","iopub.status.idle":"2025-02-07T14:00:33.971828Z","shell.execute_reply.started":"2025-02-07T14:00:27.693744Z","shell.execute_reply":"2025-02-07T14:00:33.971021Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\" to /root/.cache/torch/hub/checkpoints/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\n100%|██████████| 13.4M/13.4M [00:00<00:00, 80.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"ssdlite320_mobilenet_v3_large model is available\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y torchvision\n!pip install torchvision==0.15.2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py -O /kaggle/working/transforms.py\n\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import models\nfrom torchvision.datasets import VOCDetection\nimport torchvision.transforms.functional as F\n\nimport logging\nfrom tqdm import tqdm\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Import the downloaded utility files\nimport engine\nimport utils\nimport transforms\nimport coco_utils\nimport coco_eval\n\n# Custom classes list\nCUSTOM_CLASSES = [\n    'AllenKey', 'Axis2', 'Bearing', 'Bearing2', 'Bearing_box', \n    'Bearing_box_ax16', 'Distance_tube', 'Drill', 'Em_01', 'Em_02', \n    'F20_20_B', 'F20_20_G', 'Housing', 'M20', 'M20_100', 'M30', \n    'Motor2', 'R20', 'S40_40_B', 'S40_40_G', 'Screwdriver', \n    'Spacer', 'Wrench', 'container_box_blue', 'container_box_red'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:38:46.165860Z","iopub.execute_input":"2025-02-05T13:38:46.166177Z","iopub.status.idle":"2025-02-05T13:38:53.059297Z","shell.execute_reply.started":"2025-02-05T13:38:46.166146Z","shell.execute_reply":"2025-02-05T13:38:53.058611Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--2025-02-05 13:38:46--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8388 (8.2K) [text/plain]\nSaving to: ‘utils.py’\n\nutils.py            100%[===================>]   8.19K  --.-KB/s    in 0s      \n\n2025-02-05 13:38:46 (87.8 MB/s) - ‘utils.py’ saved [8388/8388]\n\n--2025-02-05 13:38:46--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6447 (6.3K) [text/plain]\nSaving to: ‘coco_eval.py’\n\ncoco_eval.py        100%[===================>]   6.30K  --.-KB/s    in 0s      \n\n2025-02-05 13:38:46 (61.2 MB/s) - ‘coco_eval.py’ saved [6447/6447]\n\n--2025-02-05 13:38:46--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8397 (8.2K) [text/plain]\nSaving to: ‘coco_utils.py’\n\ncoco_utils.py       100%[===================>]   8.20K  --.-KB/s    in 0s      \n\n2025-02-05 13:38:46 (81.1 MB/s) - ‘coco_utils.py’ saved [8397/8397]\n\n--2025-02-05 13:38:47--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4063 (4.0K) [text/plain]\nSaving to: ‘engine.py’\n\nengine.py           100%[===================>]   3.97K  --.-KB/s    in 0s      \n\n2025-02-05 13:38:47 (72.5 MB/s) - ‘engine.py’ saved [4063/4063]\n\n--2025-02-05 13:38:47--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 23628 (23K) [text/plain]\nSaving to: ‘/kaggle/working/transforms.py’\n\n/kaggle/working/tra 100%[===================>]  23.07K  --.-KB/s    in 0.001s  \n\n2025-02-05 13:38:47 (16.0 MB/s) - ‘/kaggle/working/transforms.py’ saved [23628/23628]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as transforms\nimport xml.etree.ElementTree as ET\nimport cv2\nimport torch.optim as optim\nimport torch.utils.data as data\n\n# Define image preprocessing transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((320, 320)),  # Resize to model's expected input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass VOCDataset(data.Dataset):\n    def __init__(self, dataset_path):\n        self.dataset_path = dataset_path\n        self.annotations = []\n        self.class_names = set()\n\n        for annotation_file in os.listdir(dataset_path):\n            if annotation_file.endswith('.xml'):\n                annotation = self._parse_annotation(os.path.join(dataset_path, annotation_file))\n                image_path = os.path.join(dataset_path, annotation['filename'])\n                if os.path.exists(image_path):\n                    annotation['image_path'] = image_path\n                    self.annotations.append(annotation)\n                    self.class_names.update(obj['name'] for obj in annotation['objects'])\n\n        self.class_names = sorted(list(self.class_names))\n        self.class_dict = {name: i+1 for i, name in enumerate(self.class_names)}\n\n    def _parse_annotation(self, annotation_path):\n        tree = ET.parse(annotation_path)\n        root = tree.getroot()\n        return {\n            'filename': root.find('filename').text,\n            'objects': [\n                {\n                    'name': obj.find('name').text,\n                    'bbox': [\n                        int(obj.find('bndbox/xmin').text),\n                        int(obj.find('bndbox/ymin').text),\n                        int(obj.find('bndbox/xmax').text),\n                        int(obj.find('bndbox/ymax').text)\n                    ]\n                } for obj in root.findall('object')\n            ]\n        }\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        annotation = self.annotations[index]\n        image = cv2.imread(annotation['image_path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Apply transformations\n        image = transform(image)\n\n        # Handle case with no objects\n        if not annotation['objects']:\n            return image, {\n                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n                'labels': torch.zeros(0, dtype=torch.int64)\n            }\n\n        boxes = torch.tensor([obj['bbox'] for obj in annotation['objects']], dtype=torch.float32)\n        labels = torch.tensor([self.class_dict[obj['name']] for obj in annotation['objects']], dtype=torch.int64)\n\n        return image, {'boxes': boxes, 'labels': labels}\n\n# Configuration\nDATASET_PATH = '/kaggle/input/rccup-voc2/ROBOCUP_OBJECTS_2024.v1-yolov3_jetson.voc/train'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset and Loader\ntrain_dataset = VOCDataset(DATASET_PATH)\ntrain_loader = data.DataLoader(\n    train_dataset, \n    batch_size=4, \n    shuffle=True, \n    drop_last=True,  # Ensure consistent batch sizes\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\n# Model Setup\nnum_classes = len(train_dataset.class_names) + 1\nmodel = torchvision.models.detection.ssdlite320_mobilenet_v3_large(num_classes=num_classes)\nmodel = model.to(device)\n\n# Training Loop\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for images, targets in train_loader:\n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        losses.backward()\n        optimizer.step()\n        \n        total_loss += losses.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n\n# Save Model\ntorch.save(model.state_dict(), \"ssdlite_mobilenet_v3_large_voc.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:33:27.132590Z","iopub.execute_input":"2025-02-06T13:33:27.132951Z","execution_failed":"2025-02-06T13:33:42.088Z"}},"outputs":[{"name":"stdout","text":"Total images: 717\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision opencv-python numpy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:39:36.270963Z","iopub.execute_input":"2025-02-07T13:39:36.271344Z","iopub.status.idle":"2025-02-07T13:39:40.113922Z","shell.execute_reply.started":"2025-02-07T13:39:36.271313Z","shell.execute_reply":"2025-02-07T13:39:40.113073Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as transforms\nimport xml.etree.ElementTree as ET\nimport cv2\nimport torch.optim as optim\nimport torch.utils.data as data\n\n# Define image preprocessing transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((320, 320)),  # Resize to model's expected input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass VOCDataset(data.Dataset):\n    def __init__(self, dataset_path):\n        self.dataset_path = dataset_path\n        self.annotations = []\n        self.class_names = set()\n\n        for annotation_file in os.listdir(dataset_path):\n            if annotation_file.endswith('.xml'):\n                annotation = self._parse_annotation(os.path.join(dataset_path, annotation_file))\n                image_path = os.path.join(dataset_path, annotation['filename'])\n                if os.path.exists(image_path):\n                    annotation['image_path'] = image_path\n                    self.annotations.append(annotation)\n                    self.class_names.update(obj['name'] for obj in annotation['objects'])\n\n        self.class_names = sorted(list(self.class_names))\n        self.class_dict = {name: i+1 for i, name in enumerate(self.class_names)}\n\n    def _parse_annotation(self, annotation_path):\n        tree = ET.parse(annotation_path)\n        root = tree.getroot()\n        return {\n            'filename': root.find('filename').text,\n            'objects': [\n                {\n                    'name': obj.find('name').text,\n                    'bbox': [\n                        int(obj.find('bndbox/xmin').text),\n                        int(obj.find('bndbox/ymin').text),\n                        int(obj.find('bndbox/xmax').text),\n                        int(obj.find('bndbox/ymax').text)\n                    ]\n                } for obj in root.findall('object')\n            ]\n        }\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        annotation = self.annotations[index]\n        image = cv2.imread(annotation['image_path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Apply transformations\n        image = transform(image)\n\n        # Handle case with no objects\n        if not annotation['objects']:\n            return image, {\n                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n                'labels': torch.zeros(0, dtype=torch.int64)\n            }\n\n        boxes = torch.tensor([obj['bbox'] for obj in annotation['objects']], dtype=torch.float32)\n        labels = torch.tensor([self.class_dict[obj['name']] for obj in annotation['objects']], dtype=torch.int64)\n\n        return image, {'boxes': boxes, 'labels': labels}\n\n# Configuration\nDATASET_PATH = '/kaggle/input/rccup-voc2/ROBOCUP_OBJECTS_2024.v1-yolov3_jetson.voc/train'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset and Loader\ntrain_dataset = VOCDataset(DATASET_PATH)\ntrain_loader = data.DataLoader(\n    train_dataset, \n    batch_size=4, \n    shuffle=True, \n    drop_last=True,  # Ensure consistent batch sizes\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\n# Model Setup\nnum_classes = 23\nmodel = torchvision.models.detection.ssdlite320_mobilenet_v3_large(num_classes=num_classes)\nmodel = model.to(device)\n\n# Training Loop\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for images, targets in train_loader:\n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        losses.backward()\n        optimizer.step()\n        \n        total_loss += losses.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n\n# Save Model\ntorch.save(model.state_dict(), \"ssdlite_mobilenet_v3_large_voc.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:41:39.559895Z","iopub.execute_input":"2025-02-07T14:41:39.560235Z","iopub.status.idle":"2025-02-07T15:07:46.510138Z","shell.execute_reply.started":"2025-02-07T14:41:39.560209Z","shell.execute_reply":"2025-02-07T15:07:46.509321Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 8.6502\nEpoch [2/10], Loss: 6.7438\nEpoch [3/10], Loss: 5.6180\nEpoch [4/10], Loss: 4.8393\nEpoch [5/10], Loss: 4.2932\nEpoch [6/10], Loss: 3.8511\nEpoch [7/10], Loss: 3.5151\nEpoch [8/10], Loss: 3.2389\nEpoch [9/10], Loss: 3.0084\nEpoch [10/10], Loss: 2.7876\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport xml.etree.ElementTree as ET\nimport cv2\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torchvision.ops.boxes import box_iou\nfrom torchvision.models.detection import ssdlite320_mobilenet_v3_large\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\n\n# Define image preprocessing transforms with Augmentations\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((320, 320)),  # Resize to model's expected input size\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass VOCDataset(data.Dataset):\n    def __init__(self, dataset_path):\n        self.dataset_path = dataset_path\n        self.annotations = []\n        self.class_names = set()\n\n        for annotation_file in os.listdir(dataset_path):\n            if annotation_file.endswith('.xml'):\n                annotation = self._parse_annotation(os.path.join(dataset_path, annotation_file))\n                image_path = os.path.join(dataset_path, annotation['filename'])\n                if os.path.exists(image_path):\n                    annotation['image_path'] = image_path\n                    self.annotations.append(annotation)\n                    self.class_names.update(obj['name'] for obj in annotation['objects'])\n\n        self.class_names = sorted(list(self.class_names))\n        self.class_dict = {name: i + 1 for i, name in enumerate(self.class_names)}\n\n    def _parse_annotation(self, annotation_path):\n        tree = ET.parse(annotation_path)\n        root = tree.getroot()\n        return {\n            'filename': root.find('filename').text,\n            'objects': [\n                {\n                    'name': obj.find('name').text,\n                    'bbox': [\n                        int(obj.find('bndbox/xmin').text),\n                        int(obj.find('bndbox/ymin').text),\n                        int(obj.find('bndbox/xmax').text),\n                        int(obj.find('bndbox/ymax').text)\n                    ]\n                } for obj in root.findall('object')\n            ]\n        }\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        annotation = self.annotations[index]\n        image = cv2.imread(annotation['image_path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Apply transformations\n        image = transform(image)\n\n        # Handle case with no objects\n        if not annotation['objects']:\n            return image, {\n                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n                'labels': torch.zeros(0, dtype=torch.int64)\n            }\n\n        boxes = torch.tensor([obj['bbox'] for obj in annotation['objects']], dtype=torch.float32)\n        labels = torch.tensor([self.class_dict[obj['name']] for obj in annotation['objects']], dtype=torch.int64)\n\n        return image, {'boxes': boxes, 'labels': labels}\n\n# Configuration\nTRAIN_DATASET_PATH = \"/kaggle/input/rccup-voc2/ROBOCUP_OBJECTS_2024.v1-yolov3_jetson.voc/train\"\nVAL_DATASET_PATH = \"/kaggle/input/rccup-voc2/ROBOCUP_OBJECTS_2024.v1-yolov3_jetson.voc/val\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load Training and Validation Datasets\ntrain_dataset = VOCDataset(TRAIN_DATASET_PATH)\nval_dataset = VOCDataset(VAL_DATASET_PATH)\n\ntrain_loader = data.DataLoader(\n    train_dataset, \n    batch_size=4, \n    shuffle=True, \n    drop_last=True,\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\nval_loader = data.DataLoader(\n    val_dataset, \n    batch_size=4, \n    shuffle=False,\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\n# Model Setup\nnum_classes = 23  # Adding 1 for background class\nmodel = ssdlite320_mobilenet_v3_large(num_classes=num_classes).to(device)\n\n# Optimizer and Scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce LR every 10 epochs\n\n# Focal Loss\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        p_t = torch.exp(-ce_loss)  # Probability of correct class\n        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nloss_fn = FocalLoss()\n\n# Function to compute Validation IoU\ndef evaluate_model(model, dataloader, device):\n    model.eval()\n    total_iou = 0\n    total_images = 0\n\n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            outputs = model(images)\n            \n            for output, target in zip(outputs, targets):\n                if len(target[\"boxes\"]) == 0 or len(output[\"boxes\"]) == 0:\n                    continue  # Skip images with no objects\n                \n                # Normalize boxes to the same scale\n                pred_boxes = output[\"boxes\"] / 320.0\n                true_boxes = target[\"boxes\"] / 320.0\n\n                iou = box_iou(pred_boxes, true_boxes)\n                max_iou, _ = iou.max(dim=1)  # Get max IoU per predicted box\n                \n                total_iou += max_iou.mean().item()  # Mean IoU per image\n                total_images += 1\n\n    return total_iou / total_images if total_images > 0 else 0\n\n# Training Loop \nfor epoch in range(50):\n    model.train()\n    total_loss = 0.0\n\n    for images, targets in train_loader:\n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        losses.backward()\n        optimizer.step()\n\n        total_loss += losses.item()\n\n    # Validation Accuracy\n    val_accuracy = evaluate_model(model, val_loader, device)\n\n    print(f\"Epoch [{epoch+1}/50], Loss: {total_loss / len(train_loader):.4f}, Validation IoU: {val_accuracy:.4f}\")\n\n    # Save model after each epoch\n    torch.save(model.state_dict(), f\"ssdlite_mobilenet_v3_large_voc_epoch{epoch+1}.pth\")\n\n    # Step the scheduler\n    scheduler.step()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies\n!pip install torch torchvision\n\n# Clone the repository containing the training script\n!git clone https://github.com/your_repo/vision_ssd_pytorch.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:15:16.746346Z","iopub.execute_input":"2025-02-05T18:15:16.746583Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nCloning into 'vision_ssd_pytorch'...\nUsername for 'https://github.com': ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport shutil\n\n# Function to validate and organize your dataset\ndef prepare_voc_dataset(source_dir, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(os.path.join(output_dir, 'Annotations'), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, 'JPEGImages'), exist_ok=True)\n    \n    # Copy and validate XML and image files\n    for filename in os.listdir(source_dir):\n        if filename.endswith('.xml'):\n            # Validate XML\n            try:\n                ET.parse(os.path.join(source_dir, filename))\n                shutil.copy(os.path.join(source_dir, filename), \n                            os.path.join(output_dir, 'Annotations', filename))\n            except ET.ParseError:\n                print(f\"Invalid XML: {filename}\")\n        \n        if filename.endswith(('.jpg', '.png', '.jpeg')):\n            shutil.copy(os.path.join(source_dir, filename), \n                        os.path.join(output_dir, 'JPEGImages', filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:25:08.341825Z","iopub.execute_input":"2025-02-05T18:25:08.342107Z","iopub.status.idle":"2025-02-05T18:25:08.347645Z","shell.execute_reply.started":"2025-02-05T18:25:08.342086Z","shell.execute_reply":"2025-02-05T18:25:08.346754Z"}},"outputs":[],"execution_count":16}]}